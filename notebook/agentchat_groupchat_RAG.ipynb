{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Group Chat with Retrieval Augmented Generation\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framwork allows tool use and human participance through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install pyautogen~=0.1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo\", \"gpt-35-turbo\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "\n",
    "llm_config = {\n",
    "    \"request_timeout\": 60,\n",
    "    \"seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "\n",
    "autogen.ChatCompletion.start_logging()\n",
    "\n",
    "raguserproxy = RetrieveUserProxyAgent(\n",
    "    name=\"raguserproxy\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    system_message=\"A human admin.\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"groupchat\",\n",
    "        \"get_or_create\": True,\n",
    "    },\n",
    "    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    ")\n",
    "\n",
    "ragcoder = RetrieveAssistantAgent(\n",
    "    name=\"ragcoder\",\n",
    "    system_message=\"You are a senior python engineer.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "   name=\"user_proxy\",\n",
    "   system_message=\"A human admin.\",\n",
    "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "   human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "\n",
    "\n",
    "def rag_chat():\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[raguserproxy, ragcoder, pm], messages=[], max_round=12\n",
    "    )\n",
    "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "    # Start chatting with raguserproxy as this is the user proxy agent.\n",
    "    raguserproxy.initiate_chat(\n",
    "        manager,\n",
    "        problem=\"How to do a regression AutoML task with FLAML and train with spark?\",\n",
    "        n_results=3,\n",
    "    )\n",
    "\n",
    "\n",
    "def norag_chat():\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[user_proxy, ragcoder, pm], messages=[], max_round=12\n",
    "    )\n",
    "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "    # Start chatting with user_proxy as this is the user proxy agent.\n",
    "    user_proxy.initiate_chat(\n",
    "        manager,\n",
    "        message=\"How to do a regression AutoML task with FLAML and train with spark?\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat\n",
    "\n",
    "### UserProxyAgent doesn't get the correct code\n",
    "[FLAML](https://github.com/microsoft/FLAML) is open sourced since 2020, so ChatGPT knows it. However, spark related APIs are added in 2022, which is not in ChatGPT's training data. As a result, we end up with wrong code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "How to do a regression AutoML task with FLAML and train with spark?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "How to do a regression AutoML task with FLAML and train with spark?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To perform a regression AutoML task with FLAML and train with Spark, you can follow these steps:\n",
      "\n",
      "1. Install FLAML and Spark on your machine.\n",
      "2. Load your data into a Spark dataframe.\n",
      "3. Create an `AutoML` object and set the `task` parameter to `'regression'`.\n",
      "4. Set the `time_budget` parameter to the maximum amount of time (in seconds) that you want FLAML to spend on the AutoML search.\n",
      "5. Call the `fit()` method on the `AutoML` object, passing in the Spark dataframe as the `data` parameter.\n",
      "6. After the `fit()` method completes, you can access the best performing model using the `best_model()` method.\n",
      "7. Use the best performing model to make predictions on new data using the `predict()` method.\n",
      "\n",
      "Here's some example code that combines FLAML and Spark to perform a regression AutoML task:\n",
      "\n",
      "```python\n",
      "from flaml import AutoML\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "# create a SparkSession\n",
      "spark = SparkSession.builder.appName(\"regression_example\").getOrCreate()\n",
      "\n",
      "# load data into a Spark dataframe\n",
      "data = spark.read.csv(\"path/to/data.csv\", header=True, inferSchema=True)\n",
      "\n",
      "# create an AutoML object for regression task\n",
      "automl = AutoML()\n",
      "automl_settings = {\n",
      "    \"time_budget\": 600  # set maximum time budget to 10 minutes\n",
      "}\n",
      "automl.set_configurations(automl_settings)\n",
      "automl.add_learner(\"rf\")  # add Random Forest as the learner\n",
      "automl.add_learner(\"xgb\")  # add XGBoost as the learner\n",
      "\n",
      "# fit AutoML using Spark dataframe\n",
      "automl.fit(\n",
      "    task=\"regression\",\n",
      "    data=data,\n",
      "    label=\"target_column_name\",\n",
      ")\n",
      "\n",
      "# retrieve best estimator and score\n",
      "best_estimator = automl.best_estimator\n",
      "best_estimator_score = automl.best_config['metric']\n",
      "\n",
      "# use best estimator to make predictions on new data\n",
      "new_data = spark.read.csv(\"path/to/new_data.csv\", header=True, inferSchema=True)\n",
      "predictions = best_estimator.predict(new_data)\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Note that the example code above assumes that your data has already been preprocessed and is ready for use with FLAML and Spark. If this is not the case, you will need to perform any necessary data cleaning, feature engineering, and/or preprocessing steps before loading the data into a Spark dataframe.\n",
      "\n",
      "Also, to use FLAML with Spark, you will need to make sure that FLAML and Spark are compatible and have compatible versions. You can check the FLAML documentation and the Spark documentation to see which versions are compatible with each other.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Additionally, it is important to keep in mind that training an AutoML model can be computationally expensive and require a lot of resources. Therefore, it is recommended to run the training process on a cluster of machines or on a cloud computing platform with sufficient resources. Furthermore, it is also important to monitor the training process and adjust the time budget and other parameters as necessary to ensure that the process completes within a reasonable amount of time while still producing useful results.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Moreover, it is advisable to evaluate the performance of the AutoML model on a holdout set or through cross-validation to ensure that it generalizes well to new data and avoid overfitting. It is also important to interpret the model's results and check for any biases or limitations that could affect its reliability or usefulness. Finally, it is crucial to document the entire process and decisions made during the development of the AutoML model, including the choice of hyperparameters, the feature selection process, and any other relevant details, to ensure that it can be replicated and improved in the future.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Absolutely, evaluation and documentation are critical steps in any machine learning project, including AutoML. Cross-validation and holdout sets help to estimate the generalization error of the model and prevent overfitting, while model interpretation can help to understand how the model is making its predictions and identify any potential biases or limitations.\n",
      "\n",
      "Documentation is also crucial for reproducibility and transparency. It allows others to understand the process you followed, reproduce your results, and build on your work. Good documentation should include details on the data used, data processing steps, feature engineering methods, hyperparameter search settings, model selection criteria, and any other relevant information. It is also important to document any assumptions or constraints made during the process and any limitations of the model.\n",
      "\n",
      "Overall, evaluation and documentation are key steps in building and deploying reliable and effective machine learning models, including AutoML models.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Is there anything else you'd like to know about using FLAML or AutoML in general?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I'm here to help with any question you might have regarding AutoML and FLAML, from data preparation and feature engineering to model selection and evaluation. Just let me know what you need and I'll do my best to assist you.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Alright, feel free to ask me any question you have about AutoML and FLAML, I'm happy to help.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "norag_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrieveUserProxyAgent get the correct code\n",
    "With RetrieveUserProxyAgent, we enabled retrieval augmented generation based on the given documentation file, ChatGPT can generate the correct code for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/anaconda3/envs/autogen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_0', 'doc_10', 'doc_4']]\n",
      "\u001b[32mAdding doc_id doc_0 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_10 to context.\u001b[0m\n",
      "\u001b[33mraguserproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: How to do a regression AutoML task with FLAML and train with spark?\n",
      "\n",
      "Context is: # Integrate - Spark\n",
      "\n",
      "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
      "- Use Spark ML estimators for AutoML.\n",
      "- Use Spark to run training in parallel spark jobs.\n",
      "\n",
      "## Spark ML Estimators\n",
      "\n",
      "FLAML integrates estimators based on Spark ML models. These models are trained in parallel using Spark, so we called them Spark estimators. To use these models, you first need to organize your data in the required format.\n",
      "\n",
      "### Data\n",
      "\n",
      "For Spark estimators, AutoML only consumes Spark data. FLAML provides a convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require.\n",
      "\n",
      "This utility function takes data in the form of a `pandas.Dataframe` or `pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a [pandas-on-spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any changes.\n",
      "\n",
      "This function also accepts optional arguments `index_col` and `default_index_type`.\n",
      "- `index_col` is the column name to use as the index, default is None.\n",
      "- `default_index_type` is the default index type, default is \"distributed-sequence\". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)\n",
      "\n",
      "Here is an example code snippet for Spark Data:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from flaml.automl.spark.utils import to_pandas_on_spark\n",
      "# Creating a dictionary\n",
      "data = {\"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
      "      \"Age_Years\": [20, 15, 10, 7, 25],\n",
      "      \"Price\": [100000, 200000, 300000, 240000, 120000]}\n",
      "\n",
      "# Creating a pandas DataFrame\n",
      "dataframe = pd.DataFrame(data)\n",
      "label = \"Price\"\n",
      "\n",
      "# Convert to pandas-on-spark dataframe\n",
      "psdf = to_pandas_on_spark(dataframe)\n",
      "```\n",
      "\n",
      "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
      "\n",
      "Here is an example of how to use it:\n",
      "```python\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "columns = psdf.columns\n",
      "feature_cols = [col for col in columns if col != label]\n",
      "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
      "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
      "```\n",
      "\n",
      "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
      "\n",
      "### Estimators\n",
      "#### Model List\n",
      "- `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.\n",
      "\n",
      "#### Usage\n",
      "First, prepare your data in the required format as described in the previous section.\n",
      "\n",
      "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
      "\n",
      "Here is an example code snippet using SparkML models in AutoML:\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "# prepare your data in pandas-on-spark format as we previously mentioned\n",
      "\n",
      "automl = flaml.AutoML()\n",
      "settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"estimator_list\": [\"lgbm_spark\"],  # this setting is optional\n",
      "    \"task\": \"regression\",\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=psdf,\n",
      "    label=label,\n",
      "    **settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
      "\n",
      "## Parallel Spark Jobs\n",
      "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](/docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
      "\n",
      "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
      "\n",
      "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
      "\n",
      "\n",
      "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
      "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
      "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
      "\n",
      "An example code snippet for using parallel Spark jobs:\n",
      "```python\n",
      "import flaml\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"n_concurrent_trials\": 2,\n",
      "    \"use_spark\": True,\n",
      "    \"force_cancel\": True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=dataframe,\n",
      "    label=label,\n",
      "    **automl_settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
      "\n",
      "<code>n_concurrent_trials</code> and <code>num_executors</code>.</li>\\n<li><code>n_concurrent_trials</code>: int, default=1 | The number of concurrent trials. When n_concurrent_trials &gt; 1, FLAML performes parallel tuning.</li>\\n<li><code>force_cancel</code>: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.</li>\\n</ul>\\n<p dir=\\\"auto\\\">An example code snippet for using parallel Spark jobs:</p>\\n<div class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import flaml\\nautoml_experiment = flaml.AutoML()\\nautoml_settings = {\\n    &quot;time_budget&quot;: 30,\\n    &quot;metric&quot;: &quot;r2&quot;,\\n    &quot;task&quot;: &quot;regression&quot;,\\n    &quot;n_concurrent_trials&quot;: 2,\\n    &quot;use_spark&quot;: True,\\n    &quot;force_cancel&quot;: True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\\n}\\n\\nautoml.fit(\\n    dataframe=dataframe,\\n    label=label,\\n    **automl_settings,\\n)\\\"><pre><span class=\\\"pl-k\\\">import</span> <span class=\\\"pl-s1\\\">flaml</span>\\n<span class=\\\"pl-s1\\\">automl_experiment</span> <span class=\\\"pl-c1\\\">=</span> <span class=\\\"pl-s1\\\">flaml</span>.<span class=\\\"pl-v\\\">AutoML</span>()\\n<span class=\\\"pl-s1\\\">automl_settings</span> <span class=\\\"pl-c1\\\">=</span> {\\n    <span class=\\\"pl-s\\\">\\\"time_budget\\\"</span>: <span class=\\\"pl-c1\\\">30</span>,\\n    <span class=\\\"pl-s\\\">\\\"metric\\\"</span>: <span class=\\\"pl-s\\\">\\\"r2\\\"</span>,\\n    <span class=\\\"pl-s\\\">\\\"task\\\"</span>: <span class=\\\"pl-s\\\">\\\"regression\\\"</span>,\\n    <span class=\\\"pl-s\\\">\\\"n_concurrent_trials\\\"</span>: <span class=\\\"pl-c1\\\">2</span>,\\n    <span class=\\\"pl-s\\\">\\\"use_spark\\\"</span>: <span class=\\\"pl-c1\\\">True</span>,\\n    <span class=\\\"pl-s\\\">\\\"force_cancel\\\"</span>: <span class=\\\"pl-c1\\\">True</span>, <span class=\\\"pl-c\\\"># Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.</span>\\n}\\n\\n<span class=\\\"pl-s1\\\">automl</span>.<span class=\\\"pl-en\\\">fit</span>(\\n    <span class=\\\"pl-s1\\\">dataframe</span><span class=\\\"pl-c1\\\">=</span><span class=\\\"pl-s1\\\">dataframe</span>,\\n    <span class=\\\"pl-s1\\\">label</span><span class=\\\"pl-c1\\\">=</span><span class=\\\"pl-s1\\\">label</span>,\\n    <span class=\\\"pl-c1\\\">**</span><span class=\\\"pl-s1\\\">automl_settings</span>,\\n)</pre></div>\\n<p dir=\\\"auto\\\"><a href=\\\"https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb\\\">Link to notebook</a> | <a href=\\\"https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb\\\" rel=\\\"nofollow\\\">Open in colab</a></p>\\n</article>\",\"renderedFileInfo\":null,\"shortPath\":null,\"tabSize\":8,\"topBannersInfo\":{\"overridingGlo\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32mAdding doc_id doc_10 to context.\u001b[0m\n",
      "\u001b[33mraguserproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: How to do a regression AutoML task with FLAML and train with spark?\n",
      "\n",
      "Context is: # Integrate - Spark\n",
      "\n",
      "FLAML has integrated Spark for distributed training. There are two main aspects of integration with Spark:\n",
      "- Use Spark ML estimators for AutoML.\n",
      "- Use Spark to run training in parallel spark jobs.\n",
      "\n",
      "## Spark ML Estimators\n",
      "\n",
      "FLAML integrates estimators based on Spark ML models. These models are trained in parallel using Spark, so we called them Spark estimators. To use these models, you first need to organize your data in the required format.\n",
      "\n",
      "### Data\n",
      "\n",
      "For Spark estimators, AutoML only consumes Spark data. FLAML provides a convenient function `to_pandas_on_spark` in the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require.\n",
      "\n",
      "This utility function takes data in the form of a `pandas.Dataframe` or `pyspark.sql.Dataframe` and converts it into a pandas-on-spark dataframe. It also takes `pandas.Series` or `pyspark.sql.Dataframe` and converts it into a [pandas-on-spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) series. If you pass in a `pyspark.pandas.Dataframe`, it will not make any changes.\n",
      "\n",
      "This function also accepts optional arguments `index_col` and `default_index_type`.\n",
      "- `index_col` is the column name to use as the index, default is None.\n",
      "- `default_index_type` is the default index type, default is \"distributed-sequence\". More info about default index type could be found on Spark official [documentation](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type)\n",
      "\n",
      "Here is an example code snippet for Spark Data:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from flaml.automl.spark.utils import to_pandas_on_spark\n",
      "# Creating a dictionary\n",
      "data = {\"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
      "      \"Age_Years\": [20, 15, 10, 7, 25],\n",
      "      \"Price\": [100000, 200000, 300000, 240000, 120000]}\n",
      "\n",
      "# Creating a pandas DataFrame\n",
      "dataframe = pd.DataFrame(data)\n",
      "label = \"Price\"\n",
      "\n",
      "# Convert to pandas-on-spark dataframe\n",
      "psdf = to_pandas_on_spark(dataframe)\n",
      "```\n",
      "\n",
      "To use Spark ML models you need to format your data appropriately. Specifically, use [`VectorAssembler`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) to merge all feature columns into a single vector column.\n",
      "\n",
      "Here is an example of how to use it:\n",
      "```python\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "columns = psdf.columns\n",
      "feature_cols = [col for col in columns if col != label]\n",
      "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
      "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
      "```\n",
      "\n",
      "Later in conducting the experiment, use your pandas-on-spark data like non-spark data and pass them using `X_train, y_train` or `dataframe, label`.\n",
      "\n",
      "### Estimators\n",
      "#### Model List\n",
      "- `lgbm_spark`: The class for fine-tuning Spark version LightGBM models, using [SynapseML](https://microsoft.github.io/SynapseML/docs/features/lightgbm/about/) API.\n",
      "\n",
      "#### Usage\n",
      "First, prepare your data in the required format as described in the previous section.\n",
      "\n",
      "By including the models you intend to try in the `estimators_list` argument to `flaml.automl`, FLAML will start trying configurations for these models. If your input is Spark data, FLAML will also use estimators with the `_spark` postfix by default, even if you haven't specified them.\n",
      "\n",
      "Here is an example code snippet using SparkML models in AutoML:\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "# prepare your data in pandas-on-spark format as we previously mentioned\n",
      "\n",
      "automl = flaml.AutoML()\n",
      "settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"estimator_list\": [\"lgbm_spark\"],  # this setting is optional\n",
      "    \"task\": \"regression\",\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=psdf,\n",
      "    label=label,\n",
      "    **settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/automl_bankrupt_synapseml.ipynb)\n",
      "\n",
      "## Parallel Spark Jobs\n",
      "You can activate Spark as the parallel backend during parallel tuning in both [AutoML](/docs/Use-Cases/Task-Oriented-AutoML#parallel-tuning) and [Hyperparameter Tuning](/docs/Use-Cases/Tune-User-Defined-Function#parallel-tuning), by setting the `use_spark` to `true`. FLAML will dispatch your job to the distributed Spark backend using [`joblib-spark`](https://github.com/joblib/joblib-spark).\n",
      "\n",
      "Please note that you should not set `use_spark` to `true` when applying AutoML and Tuning for Spark Data. This is because only SparkML models will be used for Spark Data in AutoML and Tuning. As SparkML models run in parallel, there is no need to distribute them with `use_spark` again.\n",
      "\n",
      "All the Spark-related arguments are stated below. These arguments are available in both Hyperparameter Tuning and AutoML:\n",
      "\n",
      "\n",
      "- `use_spark`: boolean, default=False | Whether to use spark to run the training in parallel spark jobs. This can be used to accelerate training on large models and large datasets, but will incur more overhead in time and thus slow down training in some cases. GPU training is not supported yet when use_spark is True. For Spark clusters, by default, we will launch one trial per executor. However, sometimes we want to launch more trials than the number of executors (e.g., local mode). In this case, we can set the environment variable `FLAML_MAX_CONCURRENT` to override the detected `num_executors`. The final number of concurrent trials will be the minimum of `n_concurrent_trials` and `num_executors`.\n",
      "- `n_concurrent_trials`: int, default=1 | The number of concurrent trials. When n_concurrent_trials > 1, FLAML performes parallel tuning.\n",
      "- `force_cancel`: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.\n",
      "\n",
      "An example code snippet for using parallel Spark jobs:\n",
      "```python\n",
      "import flaml\n",
      "automl_experiment = flaml.AutoML()\n",
      "automl_settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"task\": \"regression\",\n",
      "    \"n_concurrent_trials\": 2,\n",
      "    \"use_spark\": True,\n",
      "    \"force_cancel\": True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\n",
      "}\n",
      "\n",
      "automl.fit(\n",
      "    dataframe=dataframe,\n",
      "    label=label,\n",
      "    **automl_settings,\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "[Link to notebook](https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb) | [Open in colab](https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb)\n",
      "\n",
      "<code>n_concurrent_trials</code> and <code>num_executors</code>.</li>\\n<li><code>n_concurrent_trials</code>: int, default=1 | The number of concurrent trials. When n_concurrent_trials &gt; 1, FLAML performes parallel tuning.</li>\\n<li><code>force_cancel</code>: boolean, default=False | Whether to forcely cancel Spark jobs if the search time exceeded the time budget. Spark jobs include parallel tuning jobs and Spark-based model training jobs.</li>\\n</ul>\\n<p dir=\\\"auto\\\">An example code snippet for using parallel Spark jobs:</p>\\n<div class=\\\"highlight highlight-source-python notranslate position-relative overflow-auto\\\" dir=\\\"auto\\\" data-snippet-clipboard-copy-content=\\\"import flaml\\nautoml_experiment = flaml.AutoML()\\nautoml_settings = {\\n    &quot;time_budget&quot;: 30,\\n    &quot;metric&quot;: &quot;r2&quot;,\\n    &quot;task&quot;: &quot;regression&quot;,\\n    &quot;n_concurrent_trials&quot;: 2,\\n    &quot;use_spark&quot;: True,\\n    &quot;force_cancel&quot;: True, # Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.\\n}\\n\\nautoml.fit(\\n    dataframe=dataframe,\\n    label=label,\\n    **automl_settings,\\n)\\\"><pre><span class=\\\"pl-k\\\">import</span> <span class=\\\"pl-s1\\\">flaml</span>\\n<span class=\\\"pl-s1\\\">automl_experiment</span> <span class=\\\"pl-c1\\\">=</span> <span class=\\\"pl-s1\\\">flaml</span>.<span class=\\\"pl-v\\\">AutoML</span>()\\n<span class=\\\"pl-s1\\\">automl_settings</span> <span class=\\\"pl-c1\\\">=</span> {\\n    <span class=\\\"pl-s\\\">\\\"time_budget\\\"</span>: <span class=\\\"pl-c1\\\">30</span>,\\n    <span class=\\\"pl-s\\\">\\\"metric\\\"</span>: <span class=\\\"pl-s\\\">\\\"r2\\\"</span>,\\n    <span class=\\\"pl-s\\\">\\\"task\\\"</span>: <span class=\\\"pl-s\\\">\\\"regression\\\"</span>,\\n    <span class=\\\"pl-s\\\">\\\"n_concurrent_trials\\\"</span>: <span class=\\\"pl-c1\\\">2</span>,\\n    <span class=\\\"pl-s\\\">\\\"use_spark\\\"</span>: <span class=\\\"pl-c1\\\">True</span>,\\n    <span class=\\\"pl-s\\\">\\\"force_cancel\\\"</span>: <span class=\\\"pl-c1\\\">True</span>, <span class=\\\"pl-c\\\"># Activating the force_cancel option can immediately halt Spark jobs once they exceed the allocated time_budget.</span>\\n}\\n\\n<span class=\\\"pl-s1\\\">automl</span>.<span class=\\\"pl-en\\\">fit</span>(\\n    <span class=\\\"pl-s1\\\">dataframe</span><span class=\\\"pl-c1\\\">=</span><span class=\\\"pl-s1\\\">dataframe</span>,\\n    <span class=\\\"pl-s1\\\">label</span><span class=\\\"pl-c1\\\">=</span><span class=\\\"pl-s1\\\">label</span>,\\n    <span class=\\\"pl-c1\\\">**</span><span class=\\\"pl-s1\\\">automl_settings</span>,\\n)</pre></div>\\n<p dir=\\\"auto\\\"><a href=\\\"https://github.com/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb\\\">Link to notebook</a> | <a href=\\\"https://colab.research.google.com/github/microsoft/FLAML/blob/main/notebook/integrate_spark.ipynb\\\" rel=\\\"nofollow\\\">Open in colab</a></p>\\n</article>\",\"renderedFileInfo\":null,\"shortPath\":null,\"tabSize\":8,\"topBannersInfo\":{\"overridingGlo\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mragcoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To do a regression AutoML task with FLAML and train with Spark, you can use the `flaml.automl.spark.utils` module to convert your data into a pandas-on-spark (`pyspark.pandas`) dataframe/series, which Spark estimators require. You can then use the `lgbm_spark` estimator from the FLAML library for fine-tuning Spark version LightGBM models. Here is a code snippet for doing so:\n",
      "\n",
      "```python\n",
      "import flaml\n",
      "import pandas as pd\n",
      "from flaml.automl.spark.utils import to_pandas_on_spark\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "\n",
      "# Creating a dictionary\n",
      "data = {\"Square_Feet\": [800, 1200, 1800, 1500, 850],\n",
      "      \"Age_Years\": [20, 15, 10, 7, 25],\n",
      "      \"Price\": [100000, 200000, 300000, 240000, 120000]}\n",
      "\n",
      "# Creating a pandas DataFrame\n",
      "dataframe = pd.DataFrame(data)\n",
      "label = \"Price\"\n",
      "\n",
      "# Convert to pandas-on-spark dataframe\n",
      "psdf = to_pandas_on_spark(dataframe)\n",
      "\n",
      "# Vectorize feature columns\n",
      "columns = psdf.columns\n",
      "feature_cols = [col for col in columns if col != label]\n",
      "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
      "psdf = featurizer.transform(psdf.to_spark(index_col=\"index\"))[\"index\", \"features\"]\n",
      "\n",
      "# Define AutoML experiment settings\n",
      "automl = flaml.AutoML()\n",
      "settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": \"r2\",\n",
      "    \"estimator_list\": [\"lgbm_spark\"],\n",
      "    \"task\": \"regression\",\n",
      "}\n",
      "\n",
      "# Fit AutoML model\n",
      "automl.fit(\n",
      "    dataframe=psdf,\n",
      "    label=label,\n",
      "    **settings,\n",
      ")\n",
      "```\n",
      "\n",
      "This code first converts the input data into a pandas-on-spark dataframe using the `to_pandas_on_spark` function from `flaml.automl.spark.utils`. It then uses the `VectorAssembler` class from `pyspark.ml.feature` to convert the feature columns into a single vector column, as required by Spark estimators. The code then defines the AutoML experiment settings, including the `lgbm_spark` estimator for training Spark version LightGBM models, and trains the AutoML model on the provided data using the `fit` method of the `flaml.AutoML` class.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Another software product idea could be a machine learning model interpretability tool that helps explain the inner workings of black box models to non-experts. The tool could provide visualizations and explanations of how the model arrives at its predictions, highlighting important features and relationships between them. It could also offer suggestions for improving the model's performance and identifying potential biases or areas of concern. This tool could be useful in a variety of industries, such as healthcare and finance, where transparency and interpretability of models is increasingly important.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rag_chat()\n",
    "# type exit to terminate the chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
